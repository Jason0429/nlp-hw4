{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5313d72c-9835-45fb-bfcb-90cf977062b6",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data)\n",
    "----\n",
    "\n",
    "Due date: March 13th, 2025 @ 9pm Boston time\n",
    "\n",
    "Points: (will be listed on Canvas)\n",
    "\n",
    "Goals:\n",
    "- explore & use word embeddings\n",
    "- train neural models from the ground up!\n",
    "- get comfy with a modern neural net library (`pytorch`)\n",
    "    - you'll likelye make close friends with the docs: https://pytorch.org/tutorials/beginner/basics/intro.html \n",
    "- evaluate neural vs. vanilla n-gram language models\n",
    "\n",
    "Complete in groups of: __two (pairs)__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project!\n",
    "\n",
    "Allowed python modules:\n",
    "- `gensim`, `numpy`, `matplotlib`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, all built-in python libraries (e.g. `math` and `string`), and anything else we imported in the starter code\n",
    "- if you would like to use a library not on this list, post on piazza to request permission\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
    "\n",
    "Data processing:\n",
    "- You may __choose__ how you would like to tokenize your text for this assignment\n",
    "- You'll want to __deal with internal commas (commas inside of the sentences) appropriately__ when you read in the data, so use the python [`csv` module](https://docs.python.org/3/library/csv.html) or some other module to read the csv in (vs. splitting on commas).\n",
    "\n",
    "Warnings:\n",
    "- You might see:\n",
    "```\n",
    "notebook controller is DISPOSED. \n",
    "View Jupyter log for further details.\n",
    "```\n",
    "This is not an error per se--go to the last cell that ran successfully (or the first cell) and run them one-by-one, waiting for the prior one to finish running before moving to the next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5940a9b-f44d-49df-b0da-0170ffc6b41d",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: Jason Cheung, Robert Levin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7228d8-3ab8-4038-9f49-c4e58938fde5",
   "metadata": {},
   "source": [
    "Task 1: Provided Data Write-Up (7 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e3e2a-4db8-4554-a487-267358be5a6f",
   "metadata": {},
   "source": [
    "This is about the __provided__ ðŸŽƒ spooky ðŸ‘» authors ðŸ§Ÿ data set. Please __bold__ your answers to all written questions! Each row in this dataset represents one sentence.\n",
    "\n",
    "1. Where did you get the data from? The provided dataset is the training data from: https://www.kaggle.com/competitions/spooky-author-identification \n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)? The data was collected from text from works of fiction written by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. It was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer.\n",
    "3. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc) The data is text sentences from works of fiction written.\n",
    "4. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people) The authors of the text are Edgar Allan Poe, Mary Shelley, and HP Lovecraft.\n",
    "5. (1 pt) How large is the dataset? (# texts/sentences, # total tokens by word) (19,579 sentences, 634,080 tokens)\n",
    "6. (1 pt) What are the minimum, maximum, and average sentence lengths (by tokens) in your dataset? (min: 6, max: 878, average: 32.39)\n",
    "7. (2 pts) How large is the vocabulary, both tokenized by character and by word? (character vocabulary size: 60, word vocabulary size: 25374)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc22bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jasoncheung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jasoncheung/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30f774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 19579\n",
      "tokens: 634080\n",
      "minimum sentence length: 6\n",
      "maximum sentence length: 878\n",
      "average sentence length: 32.38571939322744\n",
      "unique character tokens: 60\n",
      "unique word tokens: 25374\n"
     ]
    }
   ],
   "source": [
    "# code that you need to answer the above questions here!\n",
    "# but make sure that you leave the answers you want us to grade in the markdown!\n",
    "\n",
    "filename = 'spooky_author_train.csv'\n",
    "\n",
    "sentences = nutils.read_file_spooky(filename, 1)\n",
    "characters = nutils.read_file_spooky(filename, 1, by_character=True)\n",
    "print(f\"sentences: {len(sentences)}\")\n",
    "print(f\"tokens: {sum([len(sentence) for sentence in sentences])}\")\n",
    "print(f\"minimum sentence length: {min([len(sentence) for sentence in sentences])}\")\n",
    "print(f\"maximum sentence length: {max([len(sentence) for sentence in sentences])}\")\n",
    "print(f\"average sentence length: {sum([len(sentence) for sentence in sentences]) / len(sentences)}\")\n",
    "print(f\"unique character tokens: {len(set([token for sentence in characters for token in sentence]))}\")\n",
    "print(f\"unique word tokens: {len(set([token for sentence in sentences for token in sentence]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e2e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
