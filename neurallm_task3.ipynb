{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8dcfc9-0378-4629-8f9f-58e64f121c3b",
   "metadata": {},
   "source": [
    "## Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634622b-b20d-43a3-a3f0-d6adc6ab64fe",
   "metadata": {},
   "source": [
    "### Names\n",
    "\n",
    "---\n",
    "\n",
    "Names: Jason Cheung, Robert Levin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47258c1-5462-4f83-afc5-2badaeb4c33d",
   "metadata": {},
   "source": [
    "## Task 3: Feedforward Neural Language Model (80 points)\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84f24a57-464a-4194-9fee-36411939e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# This function gives us nice print-outs of our models.\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff97dc8-e587-46c6-8eff-69b9c41b6099",
   "metadata": {},
   "source": [
    "### a) First, encode your text into integers (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fcca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit constants as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "OUTPUT_WORDS = 'generated_wordbased.txt' # The file to save your generated sentences for word-based LM\n",
    "OUTPUT_CHARS = 'generated_charbased.txt' # The file to save your generated sentences for char-based LM\n",
    "\n",
    "# you can update these file names if you want to depending on how you are exploring \n",
    "# hyperparameters\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "MODEL_FILE_WORD = f'spooky_author_model_word_{NGRAM}.pt' # The file to save your trained word-based neural LM to\n",
    "MODEL_FILE_CHAR = f'spooky_author_model_char_{NGRAM}.pt' # The file to save your trained char-based neural LM to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb6669b3-3df5-4d80-b67c-7c8c5f5fd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your word vectors that you made in your previous notebook AND \n",
    "# use the create_embedder function to make your pytorch embedder\n",
    "\n",
    "word_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "char_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)\n",
    "word_embedder = nutils.create_embedder(word_embeddings)\n",
    "char_embedder = nutils.create_embedder(char_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b664f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579 19579\n"
     ]
    }
   ],
   "source": [
    "# you'll also need to re-load your text data\n",
    "\n",
    "word_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM)\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "print(len(word_data), len(char_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "578103c1-6388-4f3b-abbc-54e459c1ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to vectorize a text corpus. \n",
    "# Here, it creates a mapping from word to that word's unique index.\n",
    "\n",
    "# Hint: use one of the dicts from your embedding function.\n",
    "\n",
    "def encode_tokens(data: list[list[str]], embedder: torch.nn.Embedding) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "    encoded_tokens = []\n",
    "\n",
    "    for sentence in data:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in embedder.token_to_index:\n",
    "                encoded_sentence.append(embedder.token_to_index[token])\n",
    "        encoded_tokens.append(encoded_sentence)\n",
    "\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "561ad693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 31, 2959, 0, 154, 0, 1405, 26, 43, 308, 2, 7506, 1, 2542, 2, 13, 4789, 14, 20, 8, 88, 192, 55, 4446, 0, 6, 306, 7, 1, 258, 2022, 8, 337, 84, 0, 145, 134, 905, 2, 1, 323, 14, 45, 1452, 5098, 109, 1, 442, 5, 4, 4], [3, 3, 15, 100, 135, 742, 7, 26, 12, 1, 6015, 88, 33, 9, 432, 2388, 5, 4, 4]]\n"
     ]
    }
   ],
   "source": [
    "# encode your data from tokens to integers for both word and char embeddings\n",
    "\n",
    "word_encoded = encode_tokens(word_data, word_embedder)\n",
    "char_encoded = encode_tokens(char_data, char_embedder)\n",
    "\n",
    "print(word_encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2296f18f-8f8f-41a7-85dc-b3661f12feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedder size: 25374\n",
      "Char embedder size: 60\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the mappings for each of your embedders.\n",
    "# these should match the vocab sizes you calculated in Task 2\n",
    "\n",
    "print(f\"Word embedder size: {len(word_embedder.token_to_index)}\")\n",
    "print(f\"Char embedder size: {len(char_embedder.token_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbd01e-d778-4542-8bb9-9086cdf4c06e",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556a723-7cfd-4f86-a424-0ba2e15acfb5",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences\n",
    "\n",
    "The training samples will be structured in the following format.\n",
    "Depening on which ngram model we choose, there will be (n-1) tokens\n",
    "in the input sequence (X) and we will need to predict the nth token (y).\n",
    "\n",
    "Example: this process however afforded me\n",
    "\n",
    "Would become:\n",
    "\n",
    "```\n",
    "X\n",
    "[[this,    process]\n",
    "[process, however]\n",
    "[however, afforded]]\n",
    "\n",
    "y\n",
    "[however,\n",
    "afforded,\n",
    "me]\n",
    "```\n",
    "\n",
    "Our first step is to generate n-grams like we have always been doing. We'll just do this\n",
    "on our encoded data instead of the raw text. (Feel free to consult your past HW here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9e0fc28-e667-4d72-9be2-afd749cb9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for sentence in encoded:\n",
    "        for i in range(len(sentence) - ngram + 1):\n",
    "            ngrams.append(sentence[i:i + ngram])\n",
    "\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6d80353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word sequences: 634080\n",
      "[3, 3, 31]\n",
      "[3, 31, 2959]\n",
      "[31, 2959, 0]\n",
      "[2959, 0, 154]\n",
      "[0, 154, 0]\n",
      "Char sequences: 2957553\n",
      "[25, 25, 2]\n",
      "[25, 2, 8]\n",
      "[2, 8, 6]\n",
      "[8, 6, 7]\n",
      "[6, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [0, 0, 31]\n",
    "# [0, 31, 2959]\n",
    "# [31, 2959, 2]\n",
    "# ...\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [20, 20, 2]\n",
    "# [20, 2, 8]\n",
    "# [2, 8, 6]\n",
    "# ...\n",
    "\n",
    "# print out the first 5 training samples for each and make sure that the \n",
    "# windows are sliding one word at a time. These should be integers!\n",
    "# make sure that they map to the correct words in your vocab\n",
    "# Hint: what word maps to token 0?\n",
    "\n",
    "word_training_samples = generate_ngram_training_samples(word_encoded, NGRAM)\n",
    "print(f\"Word sequences: {len(word_training_samples)}\")\n",
    "for i in range(5):\n",
    "    print(word_training_samples[i])\n",
    "\n",
    "char_training_samples = generate_ngram_training_samples(char_encoded, NGRAM)\n",
    "print(f\"Char sequences: {len(char_training_samples)}\")\n",
    "for i in range(5):\n",
    "    print(char_training_samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb275f6c-bff7-465f-b4c3-759610d44113",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a DataLoader (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "531ecf47-13ad-403d-a5e2-e19d462aab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word X shape: (634080, 2)\n",
      "Word Y length: (634080)\n",
      "Char X shape: (2957553, 2)\n",
      "Char Y length: (2957553)\n"
     ]
    }
   ],
   "source": [
    "# Note here that each sequence we've created so far is in the form:\n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate them into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here for both word and character data\n",
    "# you can write a function to do this if you'd like (not required, might be helpful)\n",
    "\n",
    "def separate_x_y(training_samples: list) -> tuple:\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for ngram in training_samples:\n",
    "        X.append(ngram[:-1])\n",
    "        y.append(ngram[-1])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "word_X, word_Y = separate_x_y(word_training_samples)\n",
    "char_X, char_Y = separate_x_y(char_training_samples)\n",
    "\n",
    "# print out the shapes (or lengths to know how many sequences there are and how many\n",
    "# elements each sub-list has) for word-based to verify that they are correct\n",
    "\n",
    "# print out the shapes for char-based to verify that they are correct\n",
    "\n",
    "print(f\"Word X shape: ({len(word_X)}, {len(word_X[0])})\")\n",
    "print(f\"Word Y length: ({len(word_Y)})\")\n",
    "print(f\"Char X shape: ({len(char_X)}, {len(char_X[0])})\")\n",
    "print(f\"Char Y length: ({len(char_Y)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bc6f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b099f7f6-be95-49f2-89e6-fa7833fcb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    X_tensor = torch.tensor(X)\n",
    "    y_tensor = torch.tensor(y)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_dataset, test_dataset = random_split(dataset, [1 - test_pct, test_pct])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ef8b",
   "metadata": {},
   "source": [
    "### some definitions:\n",
    "\n",
    "- a single **batch** is the number of sequences that your model will evaluate at once when it learns\n",
    "- **steps per epoch** is the number of batches that your model will see in a single epoch (one pass through the data)-- your NUM_SEQUENCES_PER_BATCH constant is the batch size--you won't need this for pytorch but it's useful to know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2f96706-8a23-4ca7-82ab-35d9f47a1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [128, 128]\n",
      "2 [128, 128]\n"
     ]
    }
   ],
   "source": [
    "# initialize your dataloaders for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is \n",
    "# correct for both word and character data\n",
    "# note that your train data and your test data should have the same shapes!\n",
    "# print enough information to verify that the shapes are correct\n",
    "\n",
    "word_train_loader, word_test_loader = create_dataloaders(word_X, word_Y, NUM_SEQUENCES_PER_BATCH)\n",
    "char_train_loader, char_test_loader = create_dataloaders(char_X, char_Y, NUM_SEQUENCES_PER_BATCH)\n",
    "\n",
    "# Examples:\n",
    "# Normally you would loop over your dataloader, but we just want to get a single batch to test it out:\n",
    "# Every time you call next, you advance to the next batch\n",
    "\n",
    "word_batch = next(iter(word_train_loader))\n",
    "print(len(word_batch), [len(x) for x in word_batch])\n",
    "\n",
    "char_batch = next(iter(char_train_loader))\n",
    "print(len(char_batch), [len(x) for x in char_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207dc63-8e68-47f1-b3f7-daf0c0d855f9",
   "metadata": {},
   "source": [
    "### d) Define, train & save your models (25 points)\n",
    "\n",
    "Write the code to train feedforward neural language models for both word embeddings and character embeddings make sure not to just copy + paste to train your two models (define functions as needed).\n",
    "\n",
    "Define your model architecture using PyTorch layers and activation functions. When training, use the Adam optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) instead of sgd (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD).\n",
    "\n",
    "add cells as desired :)\n",
    "\n",
    "Your FFNN should have the following architecture:\n",
    "\n",
    "- It should be a two layer neural net (one hidden layer, one output layer)\n",
    "- It should use ReLU as its activation function\n",
    "\n",
    "Our biggest piece of advice--make sure that you understand what dimensions each layer needs to be!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac10d29e-5af0-46b8-a2e6-a96832b7af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing our implementation of a Feed-Forward Neural Network.\n",
    "    You will need to implement two methods:\n",
    "        - A constructor to set up the architecture and hyperparameters of the model\n",
    "        - The forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model. \n",
    "        \n",
    "        You can change these parameters as you would like.\n",
    "        Once you get a working model, you are encouraged to\n",
    "        experiment with this constructor to improve performance.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: The number of words in the vocabulary\n",
    "            ngram: The value of N for training and prediction.\n",
    "            embedding_layer: The previously trained embedder. \n",
    "            hidden_units: The size of the hidden layer.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # we recommend saving the parameters as instance variables\n",
    "        # so you can access them later as needed\n",
    "        # (in addition to anything else you need to do here)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram = ngram\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_units = hidden_units\n",
    "        self.fc1 = nn.Linear((ngram-1) * embedding_layer.embedding_dim, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, X: list) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the network.\n",
    "        This is not a prediction, and it should not apply softmax.\n",
    "\n",
    "        Params:\n",
    "            X: the input data\n",
    "\n",
    "        Returns:\n",
    "            The output of the model; i.e. its predictions.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n",
    "        X = X.view(X.size(0), -1)\n",
    "        X = torch.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c66b6a91-2c8b-4072-815d-fd83b0a6fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Our model's training loop.\n",
    "    Print the cross entropy loss every epoch.\n",
    "    You should use the Adam optimizer instead of SGD.\n",
    "\n",
    "    When looking for documentation, try to stay on PyTorch's website.\n",
    "    This might be a good place to start: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html \n",
    "    They should have plenty of tutorials, and we don't want you to get confused from other resources.\n",
    "\n",
    "    Params:\n",
    "        dataloader: The training dataloader\n",
    "        model: The model we wish to train\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: Learning rate \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # you will need to initialize an optimizer and a loss function, which you should do\n",
    "    # before the training loop\n",
    "    model.to(torch.device('cpu'))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # print out the epoch number and the current average loss after each epoch\n",
    "    # you can use tqdm to print out a progress bar\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X, y in tqdm(dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(X)\n",
    "            \n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        tqdm.write(f\"Epoch {epoch + 1}, loss: {avg_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b981e7",
   "metadata": {},
   "source": [
    "For the next part, we're testing our model's functions so we can see if it works.\n",
    "No need to do this on both the word and character data, just one is fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0024e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FFNN                                     --\n",
       "â”œâ”€Embedding: 1-1                         (2,537,400)\n",
       "â”œâ”€Linear: 1-2                            25,728\n",
       "â”œâ”€Linear: 1-3                            3,273,246\n",
       "=================================================================\n",
       "Total params: 5,836,374\n",
       "Trainable params: 3,298,974\n",
       "Non-trainable params: 2,537,400\n",
       "================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "# Print out its architecture (use the imported summary function)\n",
    "\n",
    "model = FFNN(len(word_embedder.token_to_index), NGRAM, word_embedder)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaf2d1d4-c860-46b6-a44e-c1627a2d77f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e69cfc979d445b9a6e0ae5702896a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/3858770065.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.020865944881443\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# train your models for 1 epoch\n",
    "# see timing information posted on Canvas!\n",
    "\n",
    "# re-create your data loader fresh\n",
    "\n",
    "char_train_loader, char_test_loader = create_dataloaders(char_X, char_Y, NUM_SEQUENCES_PER_BATCH)\n",
    "\n",
    "# train your model\n",
    "\n",
    "model = FFNN(len(char_embedder.token_to_index), NGRAM, char_embedder)\n",
    "train(char_train_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755443f",
   "metadata": {},
   "source": [
    "10. You're reporting the loss after each epoch of training. What is the loss for your model after 1 epoch?\n",
    "\n",
    "- word or character-based? **Word**\n",
    "- loss? **5.8**\n",
    "\n",
    "Loss isn't accuracy, but it does tell us whether or not the model is improving over time. For character-based, loss after one epoch should be ~2.1; for word-based it is ~5.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31e2d",
   "metadata": {},
   "source": [
    "### e) create a full pipeline (13 points)\n",
    "\n",
    "We've made all the pieces that you'll need for a full pipeline, now let's package everything together nicely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b22dfd78-fc75-41eb-ba67-17b1141ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "# make a function that does your full *training* pipeline\n",
    "# This is essentially pulling the pieces that you've done so far earlier in this \n",
    "# notebook into a single function that you can call to train your model\n",
    "\n",
    "\n",
    "def full_pipeline(data: list[list[str]], word_embeddings_filename: str, \n",
    "                batch_size:int = NUM_SEQUENCES_PER_BATCH,\n",
    "                ngram:int = NGRAM, hidden_units = 128, epochs = 1,\n",
    "                lr = 0.001, test_pct = 0.1,\n",
    "                ) -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from loading embeddings to training.\n",
    "    You won't use the test set for anything.\n",
    "\n",
    "    Params:\n",
    "        data: The raw data to train on, parsed as a list of lists of tokens\n",
    "        word_embeddings_filename: The filename of the Word2Vec word embeddings\n",
    "        batch_size: The batch size to use\n",
    "        hidden_units: The number of hidden units to use\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: The learning rate to use\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    embeddings = nutils.load_word2vec(word_embeddings_filename)\n",
    "    embedder = nutils.create_embedder(embeddings)\n",
    "    \n",
    "    encoded = encode_tokens(data, embedder)\n",
    "    training_samples = generate_ngram_training_samples(encoded, ngram)\n",
    "    X, y = separate_x_y(training_samples)\n",
    "    train_loader, _ = create_dataloaders(X, y, batch_size, test_pct)\n",
    "    \n",
    "    model = FFNN(len(embedder.token_to_index), ngram, embedder, hidden_units)\n",
    "    train(train_loader, model, epochs, lr)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24546581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f4ec8f4bc2403a96e8591ca57a1689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/3858770065.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 6.266454542129663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb53990a2e654794b21ac94b4f03baa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 5.631653323518933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6678019da8564a7abff89856520ecd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 5.456447051335514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af209c9e5b264929a53c6bbdec7951ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 5.3520171268554675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cecc3198c8b40b4940958896fe55f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 5.2733773479388955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c692911ece44c92a4f33d1c586ee4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 5.209107867910553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b62658de744015b02a36a64f9b1aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 5.154421379866154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28fe95d29bf426db77fef59e38eb376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/4459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 5.106474551525745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6e19196dae4581b61a87900905248f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/3858770065.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.195897299014598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab64210a80e45c6a0ce8046885c5722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 2.053787588603131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6134dc8cc6964685a58199fbd70261b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 2.0208462898686292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e933344d328547549f9a80c96b6624e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 2.003250498230766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9bc48448c4491aae48986b847a38ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 1.9922748166085391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c068e150756492c8ee8e0b5dc21d8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 1.984523223286341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043c88d3e6348eab310605b0c955f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 1.9785640546681216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3bc35c5fd94a5782736d287d080f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/20796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 1.9738219541514188\n"
     ]
    }
   ],
   "source": [
    "# 10 points\n",
    "\n",
    "# Use your full pipeline to train models on the word data and the character data.\n",
    "# Feel free to add cells if you'd like to.\n",
    "\n",
    "# Train your models however you'd like. Play around with number of epochs, learning rate, etc.\n",
    "# Do whatever you'd like to for exploring hyperparameters.\n",
    "# You aren't required to hit a certain loss, but you should leave code here that shows\n",
    "# that you explored effects of changing at least two of the different hyperparameters\n",
    "# Please don't change the architecture of the model (keep it a 2-layer model with 1 hidden layer)\n",
    "\n",
    "# You'll likely want to do this exploration AFTER completing your prediction and generation code, so start\n",
    "# with just training for 1 - 5 epochs with default params.\n",
    "\n",
    "\n",
    "# Word-based takes Felix's computer 7 - 8 min for 5 epochs with default params running on CPU\n",
    "# Char-based Felix's computer ~1min 30sec - 2min for 5 epochs with default params running on CPU\n",
    "\n",
    "# TODO: uncomment\n",
    "LEARNING_RATE = 0.0001\n",
    "word_model = full_pipeline(word_data, EMBEDDING_SAVE_FILE_WORD, epochs=8, lr=LEARNING_RATE)\n",
    "char_model = full_pipeline(char_data, EMBEDDING_SAVE_FILE_CHAR, epochs=8, lr=LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1171c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you're happy with them, save both models\n",
    "# Feel free to play around with any hyperparameters you'd like\n",
    "\n",
    "# using torch.save and the model's state_dict\n",
    "\n",
    "torch.save(word_model.state_dict(), MODEL_FILE_WORD)\n",
    "torch.save(char_model.state_dict(), MODEL_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e554fe9-a413-4471-96fe-a51e3764c2ae",
   "metadata": {},
   "source": [
    "### f) Generate Sentences (25 points)\n",
    "\n",
    "Now that you have trained models, you'll work on the generation piece. Note that because you saved your models, even if you have to re-start your kernel, you should be able to re-load them without having to re-train them again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad01dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/2847720804.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_FILE_WORD))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (embedding_layer): Embedding(25374, 100)\n",
       "  (fc1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=25374, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the models in again with code like:\n",
    "model = FFNN(len(word_embedder.token_to_index), NGRAM, word_embedder)\n",
    "model.load_state_dict(torch.load(MODEL_FILE_WORD))\n",
    "# then switch the model into evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "608b05fb-8db9-4d44-8cb2-bb9ec5f373d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points \n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    with torch.no_grad():\n",
    "        expected_length = NGRAM - 1\n",
    "        if len(input_tokens) < expected_length:\n",
    "            input_tokens = [\"<s>\"] * (expected_length - len(input_tokens)) + input_tokens\n",
    "        \n",
    "        input_indices = [word_embedder.token_to_index[token] for token in input_tokens]\n",
    "        input_tensor = torch.tensor([input_indices])\n",
    "        output = model(input_tensor)\n",
    "        probs = torch.softmax(output, dim=-1)\n",
    "        predicted_index = torch.multinomial(probs[0], num_samples=1).item()\n",
    "        predicted_token = word_embedder.index_to_token[predicted_index]\n",
    "        \n",
    "    return predicted_token\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f721708d-3dde-4e91-888d-080c4ac6ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: list[str], max_tokens: int = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\"\n",
    "    context_length = NGRAM - 1\n",
    "\n",
    "    while max_tokens is None or max_tokens > 0:\n",
    "        # Use only the last (n-1) tokens as context\n",
    "        context = seed[-context_length:]\n",
    "        \n",
    "        # Get the next token prediction\n",
    "        next_token = predict(model, context)\n",
    "        seed.append(next_token)\n",
    "        \n",
    "        # Check for end-of-sentence token. (Assuming \"</s>\" is your EOS marker.)\n",
    "        if next_token == \"</s>\":\n",
    "            break\n",
    "        \n",
    "        if max_tokens is not None:\n",
    "            max_tokens -= 1\n",
    "\n",
    "    return seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0219cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f46b317a-e8c7-41d4-a598-34318ffda93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Model Outputs:\n",
      "`` but the thing began while a short line of a gifted success .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/3858770065.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what 's most began other wind reached aim was opened , it the future steeps and we found , but then in raymond .\n",
      "there would be no has on the powers strange opposite and flower singly , till our visitors is not of the chasms gasped , '' , `` hans '' than a moment in his side , disjointed now get a inclination that door of no for which i had raised myself been before , column , i reached a consequence in the meantime ; although brought has failed whom while the garment of the solution temple in oak hard forever , half books destroyed onward this to reach in a character hymn upon the mere frost .\n",
      "heard , talking beings that so maintained a solution of your very dream through the intruder of foamless the idol did not even longer why suffered , saying to prevent all creatures features '' and soon men , streamed , the boy of the earth ; for his notes for , in ten .\n",
      "i accustomed lost ; i crawled a tall of rotting match .\n",
      "we insisted , for her , , was that aroused i had thus explain much explained in mentioned appears west , and mine to what had been pain as we frequently on it , i called and the theories of the water place , and , certainly to your appear .\n",
      "the being descend ; but i realise the matter , to mutter and across her notice hand , carrington , beneath hermann and that i can not have overlooked through his expense ; and the countryside 's wrist , and went eyes , hope a only for ingolstadt whether by the outspread of l'etoile .\n",
      "perdita had given certain bob down , monotonous returned me up i saw no very and smelt .\n",
      "one regarded which she turned let you , and wrote us courage by ever now ventured arthur and prattle of athens yxur , and life of the woods i began the same memorable a almost lights and ease that affords shadow .\n",
      "sir holland is obviously spirit did not have been constantly to her , and were , was all ; it would be brought .\n",
      "\n",
      "Character Model Outputs:\n",
      ".ofand\n",
      "in,ofithe,\n",
      ".and.withtoas,heandwastoitheahiswaswas,iand;wasof,.in,ita\n",
      ",his;;wasthein,;\n",
      "to,andtoofand.with,myhadas,heandofas,and.ofto,heiandwas,.me,\n",
      "a,tothatathe,toandnottheinwhichbutbutforinthetheofthatandoftoasby,ithe,inthattoandof,thein,i\n",
      "me,to\n",
      "to,it\n",
      "wasand.with,myofoftheaitand.,to,myforwasthe.,hisa\n",
      ".in,ofithe,the.in,;\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# generate and display ten sequences from both your word model and your character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# For character-based, replace _ with a space\n",
    "\n",
    "print(\"Word Model Outputs:\")\n",
    "for i in range(10):\n",
    "    word_sequence = generate(word_model, [\"<s>\"])\n",
    "\n",
    "    filtered_tokens = [token for token in word_sequence if token not in {\"<s>\", \"</s>\"}]\n",
    "\n",
    "    sentence = \" \".join(filtered_tokens)\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nCharacter Model Outputs:\")\n",
    "for i in range(10):\n",
    "    char_sequence = generate(char_model, [\"<s>\"])\n",
    "\n",
    "    filtered_chars = [token for token in char_sequence if token not in {\"<s>\", \"</s>\"}]\n",
    "\n",
    "    sentence = \"\".join(filtered_chars)\n",
    "\n",
    "    sentence = sentence.replace(\"_\", \" \")\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "703a1ec4-9a8b-46dc-894d-b41be323058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/_5tz9gnd2kd70vc85hntn3600000gn/T/ipykernel_84792/3858770065.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.stack([self.embedding_layer(torch.tensor(x)) for x in X], dim=0)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Generate 100 example sentences with each model and save them to two files, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "# We've defined the filenames for you at the top of this notebook\n",
    "# Do not print these sentences here :)\n",
    "\n",
    "for i in range(100):\n",
    "    word_sequence = generate(word_model, [\"<s>\"], 20)\n",
    "    filtered_words = [token for token in word_sequence if token not in {\"<s>\", \"</s>\"}]\n",
    "    word_sentence = \" \".join(filtered_words)\n",
    "    \n",
    "    char_sequence = generate(char_model, [\"<s>\"], 20)\n",
    "    filtered_chars = [token for token in char_sequence if token not in {\"<s>\", \"</s>\"}]\n",
    "    char_sentence = \"\".join(filtered_chars)\n",
    "    char_sentence = char_sentence.replace(\"_\", \" \")\n",
    "    \n",
    "    with open(OUTPUT_WORDS, \"a\") as file:\n",
    "        file.write(word_sentence + \"\\n\")\n",
    "    with open(OUTPUT_CHARS, \"a\") as file:\n",
    "        file.write(char_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef7860",
   "metadata": {},
   "source": [
    "11. What were the final parameters that you used for your model?\n",
    "\n",
    "- N: **3**\n",
    "- embedding size: **50**\n",
    "- epochs: **8**\n",
    "- hidden units: **128**\n",
    "- learning rate: **0.0001**\n",
    "- training time + system you were running it on (operating system + chip/specs): **8 mins. Windows 11, intel core i7-12650H 2.3GHZ**\n",
    "\n",
    "  - for pairs, you can either note both partners' training times or just one\n",
    "\n",
    "- What was the word-based model's final loss? **5.1**\n",
    "- Character based? **1.97**\n",
    "\n",
    "If you used different parameters for your word-based and character-based models, note the different parameters clearly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
