{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b4dd3e-f5e5-4671-a86a-82372ac1b0fc",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927209b-31e2-4f54-a95a-fbaf7e5f00a3",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: Jason Cheung, Robert Levin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25ecf4-4b08-4d35-b150-5db899662272",
   "metadata": {},
   "source": [
    "Task 2: Training your own word embeddings (15 points)\n",
    "--------------------------------\n",
    "\n",
    "For this task, you'll use the `gensim` package to train your own embeddings for both words and characters. These will eventually act as inputs to your neural language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are several dependencies to install\n",
    "# !python --version\n",
    "# !python -m pip install --upgrade pip setuptools wheel\n",
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# !pip install torch torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd110e8-9404-4bf3-b6c2-27120adfd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jasoncheung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jasoncheung/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "# for word embeddings\n",
    "# if not installed, run the following command:\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aa8e0-2dce-4d33-8312-4b2b0146dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on google colab, you'll need to mount your drive to access data files\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ce4aff-bba6-429d-b618-c57c23b350fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "\n",
    "# The dimensions of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# DO NOT WRITE \"50\" WHEN YOU ARE REFERRING TO THE EMBEDDING SIZE\n",
    "EMBEDDINGS_SIZE = 50\n",
    "\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb948-4d52-41b4-a3e6-4b77d4232032",
   "metadata": {},
   "source": [
    "Train embeddings on provided dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d77d3-1d87-4cd4-a9b6-acbed5d2c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'i', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'i', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.', '</s>'], ['<s>', 'it', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.', '</s>']]\n",
      "[['<s>', 't', 'h', 'i', 's', '_', 'p', 'r', 'o', 'c', 'e', 's', 's', ',', '_', 'h', 'o', 'w', 'e', 'v', 'e', 'r', ',', '_', 'a', 'f', 'f', 'o', 'r', 'd', 'e', 'd', '_', 'm', 'e', '_', 'n', 'o', '_', 'm', 'e', 'a', 'n', 's', '_', 'o', 'f', '_', 'a', 's', 'c', 'e', 'r', 't', 'a', 'i', 'n', 'i', 'n', 'g', '_', 't', 'h', 'e', '_', 'd', 'i', 'm', 'e', 'n', 's', 'i', 'o', 'n', 's', '_', 'o', 'f', '_', 'm', 'y', '_', 'd', 'u', 'n', 'g', 'e', 'o', 'n', ';', '_', 'a', 's', '_', 'i', '_', 'm', 'i', 'g', 'h', 't', '_', 'm', 'a', 'k', 'e', '_', 'i', 't', 's', '_', 'c', 'i', 'r', 'c', 'u', 'i', 't', ',', '_', 'a', 'n', 'd', '_', 'r', 'e', 't', 'u', 'r', 'n', '_', 't', 'o', '_', 't', 'h', 'e', '_', 'p', 'o', 'i', 'n', 't', '_', 'w', 'h', 'e', 'n', 'c', 'e', '_', 'i', '_', 's', 'e', 't', '_', 'o', 'u', 't', ',', '_', 'w', 'i', 't', 'h', 'o', 'u', 't', '_', 'b', 'e', 'i', 'n', 'g', '_', 'a', 'w', 'a', 'r', 'e', '_', 'o', 'f', '_', 't', 'h', 'e', '_', 'f', 'a', 'c', 't', ';', '_', 's', 'o', '_', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '_', 'u', 'n', 'i', 'f', 'o', 'r', 'm', '_', 's', 'e', 'e', 'm', 'e', 'd', '_', 't', 'h', 'e', '_', 'w', 'a', 'l', 'l', '.', '</s>'], ['<s>', 'i', 't', '_', 'n', 'e', 'v', 'e', 'r', '_', 'o', 'n', 'c', 'e', '_', 'o', 'c', 'c', 'u', 'r', 'r', 'e', 'd', '_', 't', 'o', '_', 'm', 'e', '_', 't', 'h', 'a', 't', '_', 't', 'h', 'e', '_', 'f', 'u', 'm', 'b', 'l', 'i', 'n', 'g', '_', 'm', 'i', 'g', 'h', 't', '_', 'b', 'e', '_', 'a', '_', 'm', 'e', 'r', 'e', '_', 'm', 'i', 's', 't', 'a', 'k', 'e', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# use the provided utility functions to read in the data\n",
    "\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "\n",
    "# read the spooky data in both by character and by word using the read_file_spooky function in the \n",
    "# provided utils\n",
    "data_by_word = nutils.read_file_spooky(TRAIN_FILE, 1)\n",
    "data_by_char = nutils.read_file_spooky(TRAIN_FILE, 1, by_character=True)\n",
    "\n",
    "# print out the first two sentences in each format\n",
    "# make sure we can read the output easily without scrolling to the side too much\n",
    "print(data_by_word[:2])\n",
    "print(data_by_char[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704033c",
   "metadata": {},
   "source": [
    "8. What character represents spaces when we tokenize by character? __YOUR ANSWER HERE__\n",
    "9. Read the word2vec documentation. What do the following parameters signify?\n",
    "    - embeddings_size: __YOUR ANSWER HERE__\n",
    "    - window: __YOUR ANSWER HERE__\n",
    "    - min_count: __YOUR ANSWER HERE__\n",
    "    - sg: __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33920ce1-faee-4774-8aae-d78e511bf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "# create your word embeddings\n",
    "# use the skip gram algorithm and a window size of 5\n",
    "# min_count should be 1\n",
    "# takes ~3.3 sec on Felix's computer for character embeddings using skip-gram with window size 5\n",
    "# takes ~3.3 sec on Felix's computer for word embeddings using skip-gram with window size 5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_word2vec(data: list[list[str]], embeddings_size: int,\n",
    "                    window: int = 5, min_count: int = 1, sg: int = 1) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Create new word embeddings based on our data.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A gensim Word2Vec model\n",
    "        https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    \"\"\"\n",
    "\n",
    "    return Word2Vec(sentences=data, window=5, min_count=1)\n",
    "\n",
    "\n",
    "# After you are happy with this function, copy + paste it into the bottom of \n",
    "# your neurallm_utils.py file\n",
    "# You'll need it for the next task!\n",
    "def create_embedder(raw_embeddings: Word2Vec) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    Create a PyTorch embedding layer based on our data.\n",
    "\n",
    "    We will *first* train a Word2Vec model on our data.\n",
    "    Then, we'll use these weights to create a PyTorch embedding layer.\n",
    "        `nn.Embedding.from_pretrained(weights)`\n",
    "\n",
    "\n",
    "    PyTorch docs: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained\n",
    "    Gensim Word2Vec docs: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "    Pay particular attention to the *types* of the weights and the types required by PyTorch.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint:\n",
    "    # For later tasks, we'll need two mappings: One from token to index, and one from index to tokens.\n",
    "    # It might be a good idea to store these as properties of your embedder.\n",
    "    # e.g. `embedder.token_to_index = ...`\n",
    "    \n",
    "    word_vectors = raw_embeddings.wv\n",
    "    vocab = word_vectors.index_to_key\n",
    "    token_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    index_to_token = {idx: word for word, idx in token_to_index.items()}\n",
    "    weights = torch.FloatTensor(np.array([word_vectors[word] for word in vocab]))\n",
    "\n",
    "    embedder = nn.Embedding.from_pretrained(weights)\n",
    "    embedder.token_to_index = token_to_index\n",
    "    embedder.index_to_token = index_to_token\n",
    "\n",
    "    return embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db8e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and save both sets (word and character based) of Word2Vec embeddings. \n",
    "# Use the provided utility functions in nutils.\n",
    "# These will be (re)loaded in the next notebook.\n",
    "\n",
    "embeddings_by_word = train_word2vec(data_by_word, EMBEDDINGS_SIZE)\n",
    "embeddings_by_char = train_word2vec(data_by_char, EMBEDDINGS_SIZE)\n",
    "nutils.save_word2vec(embeddings_by_word, EMBEDDING_SAVE_FILE_WORD)\n",
    "nutils.save_word2vec(embeddings_by_char, EMBEDDING_SAVE_FILE_CHAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c85728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=25374, vector_size=100, alpha=0.025>\n",
      "Word2Vec<vocab=60, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# load them in again to make sure that this works and is still fast\n",
    "\n",
    "embeddings_by_word = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "embeddings_by_char = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)\n",
    "print(embeddings_by_word)\n",
    "print(embeddings_by_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57301fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the embedders\n",
    "\n",
    "embedder_by_word = create_embedder(embeddings_by_word)\n",
    "embedder_by_char = create_embedder(embeddings_by_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d7c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      ",: 0\n",
      "the: 1\n",
      "of: 2\n",
      "<s>: 3\n",
      "</s>: 4\n",
      ".: 5\n",
      "and: 6\n",
      "to: 7\n",
      "i: 8\n",
      "a: 9\n",
      "----------------------\n",
      "----------------------\n",
      "0: ,\n",
      "1: the\n",
      "2: of\n",
      "3: <s>\n",
      "4: </s>\n",
      "5: .\n",
      "6: and\n",
      "7: to\n",
      "8: i\n",
      "9: a\n",
      "----------------------\n",
      "----------------------\n",
      "_: 0\n",
      "e: 1\n",
      "t: 2\n",
      "a: 3\n",
      "o: 4\n",
      "n: 5\n",
      "i: 6\n",
      "s: 7\n",
      "h: 8\n",
      "r: 9\n",
      "----------------------\n",
      "----------------------\n",
      "0: _\n",
      "1: e\n",
      "2: t\n",
      "3: a\n",
      "4: o\n",
      "5: n\n",
      "6: i\n",
      "7: s\n",
      "8: h\n",
      "9: r\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# take a look at your saved token to index and index to token mappings in your embedders to make sure they make sense\n",
    "# AND that they are both dictionaries mapping from int to str or vice versa!\n",
    "# don't leave a ton of output in your notebook when you turn it in, but you need to understand this,\n",
    "# and it's an easy place to make a mistake that's hard to debug later.\n",
    "# do leave whatever code you use here, comment it out if it produces a lot of output\n",
    "\n",
    "def pretty_print(d: dict, n: int = None):\n",
    "    if n is None:\n",
    "        n = len(d)\n",
    "\n",
    "    print(\"----------------------\")\n",
    "    for k, v in list(d.items())[:n]:\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "pretty_print(embedder_by_word.token_to_index, n=10)\n",
    "pretty_print(embedder_by_word.index_to_token, n=10)\n",
    "pretty_print(embedder_by_char.token_to_index, n=10)\n",
    "pretty_print(embedder_by_char.index_to_token, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb66cea2-746e-4371-8966-33dd358d46c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Vocabulary Size: 25374\n",
      "Character Embeddings Vocabulary Size: 60\n"
     ]
    }
   ],
   "source": [
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n",
    "\n",
    "print(\"Word Embeddings Vocabulary Size:\", len(embedder_by_word.token_to_index))\n",
    "print(\"Character Embeddings Vocabulary Size:\", len(embedder_by_char.token_to_index))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
